<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Basil N. Saeed</title>
  
  <meta name="author" content="Basil N. Saeed">
  <meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="robots" content="noindex, nofollow" />
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Basil N. Saeed</name>
                <br>
                bsaeed [at] stanford.edu  
								&nbsp/&nbsp
								<a href="https://scholar.google.com/citations?user=CZ3EC4AAAAAJ&hl=en">Google Scholars</a>
              </p>
              <p>
                I'm a PhD candidate at Stanford, working with Professor 
                <a href="https://web.stanford.edu/~montanar/">Andrea Montanari</a>.
                 I'm currently interested in high-dimensional probability and statistics theory, and its application 
                as a foundation for machine learning.
              </p>

              <p>Previously, I was a Master's student at MIT, where I worked in the <a href="https://idss.mit.edu/">MIT Institute for Data, Systems, and Society (IDSS)</a> with Professor <a href="https://www.carolineuhler.com">Caroline Uhler</a> on causal inference and graphical models.
              </p>
              <p>
              I graduated from MIT with a B.S. in Computer Science and Electrical Engineering with a minor in Mathematics.  
							Previously, I worked in the MIT Computational Cognitive Science Lab with Professors 
							<a href="https://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a> and
							<a href="https://statistics.yale.edu/seminars/ilker-yildirim">Ilker Yildirim</a>,
							and the Laboratory for Information and Decision Systems (LIDS) with Professor <a href="https://idss.mit.edu/staff/robert-c-berwick/">Bob Berwick</a>.
              </p>
              <p>
                I was fortunate to receive support from the NSF Graduate Research Fellowship Program for my PhD.
              </p>

              <!--<p style="text-align:center">
                <a href="mailto:bsaeed@stanford.edu">Email</a> 
              </p>-->
								<!--&nbsp/&nbsp
								<a href="data/BasilSaeedCV.pdf">CV</a>-->
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/BasilSaeed.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/BasilSaeed_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
              <h3>High-dimensional kernels and random matrix theory</h3>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/kernel.png" alt="anchored-ci" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle> A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator
                </papertitle>
              <br>
              Theodor Misiakiewicz, <strong>Basil Saeed</strong>
              <br>
              <em>Under Review</em>
              <p>
                We study kernel ridge regression in the high-dimensional polynomial regime, and derive a non-asymptotic characterization for the test error and the GCV estimator.
                This is done via the theory of deterministic equivalents in "infinite-dimensional" random matrix theory.
                              <br>
							[<a href="https://arxiv.org/abs/2202.08832">Link</a>]
							</p>
						</td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
              <h3>Universality and Invariance Principles in high-dimensions</h3>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/univ1.png" alt="anchored-ci" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Universality of Empirical Risk Minimization</papertitle>
              <br>
              Andrea Montanari, <strong>Basil Saeed</strong>
              <br>
              <em>Under Review</em>
              <p>
                We give relatively general conditions on an empirical risk minimization problem under which the test and train error of the resulting estimator asymptotically depends only on the first and second moments of the 
                data distribution. This allows one to analyze a "Gaussian equivalent" model where the data is replaced with Gaussian data with matching first and second moments.
              <br>
							[<a href="https://arxiv.org/abs/2202.08832">Link</a>]
							</p>
						</td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/univ2.png" alt="anchored-ci" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Universality of max-margin classifiers</papertitle>
              <br>
              Andrea Montanari, Feng Ruan, <strong>Basil Saeed</strong>, Youngtak Sohn
              <br>
              <em>Under Review</em>
              <p>
                We extend universality to the min-max extremal problem of max-margin classification; namely, we show that the margin and the classification error asymptotically
                 depends on the data only through the first and second moments of the distribution. We show that this phenomenon holds due to a hidden averaging effect:
                 in the high-dimensional regime, the max-margin classifier is a maximization of an ERM-like problem where the average is over order sample-size many support vectors.                              <br>
							[<a href="https://arxiv.org/abs/2310.00176">Link</a>]
							</p>
						</td>
          </tr>




        </tbody></table>












        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
              <h3>Graphical Models and Causal Discovery</h3>
            </td>
          </tr>
        </tbody></table>





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/gspo.jpg" alt="poset" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Learning Directed Graphical Models with Latent Variables</papertitle>
              <br>
              <em>Master's Thesis</em>
              <p>
							Develops a provably consistent score-based algorithm for causal discovery in the presence of latent confounders: given observed data from a mixed graph (representing a causal graph with latent confounders), the algorithm maps 
							to every poset the mixed graph that is most representative of the data among the ones compatible with the poset, and greedily searches
							over the more constrained space of posets to find a graph that is Markov equivalent to the generating graph.
              <br>
							[<a href="https://dspace.mit.edu/handle/1721.1/127515">Link</a>]
							</p>
						</td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/mixture.png" alt="mixture" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Causal Structure Discovery from Distributions Arising from Mixtures DAGs</papertitle>
              <br>
							with <a href="https://web.stanford.edu/~snigdha/">Snigdha Panigrahi</a> and 
							<a href="https://www.carolineuhler.com">Caroline Uhler</a>
              <br>
              <em>AISTATS 2020</em>
              <p>
							We provide theoretical guarantees on what can be learned from data generated from a mixture of DAGs, with limited knowledge about the mixture components 
							and the mixture proportions. We investigate what can be said about the components of the mixture and the membership of the data-points.
              <br>
							[<a href="https://proceedings.mlr.press/v119/saeed20a/saeed20a.pdf">Link</a>]
							</p>
						</td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/orderingbased.jpg" alt="poset" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Ordering-Based Causal Inference in the Presence of Latent Variables</papertitle>
              <br>
							<a href="https://dibernstein.github.io/"> Daniel Bernstein*</a>, <strong>Basil Saeed*</strong>, <a href="https://github.com/csquires"> Chandler Squires*</a>, and 
							<a href="https://www.carolineuhler.com">Caroline Uhler</a>
              <br>
              <em>ICML 2020</em>
              <p>
							We show that learning a causal graph in the presence of latent variables (represented by mixed graphs),
							can be cast as an optimization problem over the space of partial orderings of the set of observed variables.
							We prove under assumptions weaker than faithfulness of the distribution to a mixed graph that any sparsest
							independence map (IMAP) of the distribution belongs to the Markov equivalence class of
							the true model. This motivates the Sparsest Poset formulation - that posets can be mapped
							to minimal IMAPs of the true model such that the sparsest of these IMAPs is Markov
							equivalent to the true model.
              <br>
							[<a href="https://arxiv.org/pdf/1910.09014.pdf"> Link </a>]
							</p>
						</td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/anchored_ci.png" alt="anchored-ci" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Anchored Causal Inference in the Presence of Measurement Error</papertitle>
              <br>
              <strong>Basil Saeed</strong>, Anastasiya Belyaeva, Yuhao Wang, Caroline Uhler
              <br>
              <em>UAI 2020</em>
              <p>We develop a provably consistent procedure for learning a causal graph in the 
							presence of measurement error for a wide class of measurement noise models when the 
							noiseless variables are Gaussian. 
							We prove asymptotic consistency, discuss finite-sample considerations and demonstrate 
							our method's performance on simulated and real data to recover the underlying gene 
							regulatory network from zero-inflated single-cell RNA-seq data. 
							<br>
							[<a href="https://arxiv.org/abs/1906.00928">Link</a>]
							<!--[<a href="https://github.com/basilnsaeed/anchored_causal_inference">CODE</a>]-->
							</p>
						</td>
          </tr>











        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
              <h3>Task and motion plaining in physical problem solving, and its relation with the human concept of ``effort''</h3>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/intuitive_difficulty.png" alt="intuitive-difficulty" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Explaining intuitive difficulty judgments by modeling physical effort and risk</papertitle>
         
              <br>
              Ilker Yildirim*, <strong>Basil Saeed*</strong>, Grace Bennett-Pierre, Tobias Gerstenberg, Joshua Tenenbaum, Hyowon Gweon
              <br>
              <em>CogSci 2019</em>
              <p>We give a computational account of how humans judge the difficulty of a range of physical construction tasks 
							(e.g., moving 10 loose blocks from their initial configuration to their target configuration, such as a vertical tower)
							by quantifying two key factors that influence construction difficulty: physical effort and physical risk.
							<br>
							[<a href="https://arxiv.org/abs/1905.04445">Link</a> ]
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="display: block;margin-left: auto;margin-right: auto;" src="images/joint_planning.png" alt="joint-planning" width="300">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints</papertitle>
              <br>
              Ilker Yildirim*, Tobias Gerstenberg*, <strong>Basil Saeed</strong>, Marc Toussaint, Josh Tenenbaum
              <br>
              <em>CogSci 2017</em>
              <p>We develop a model that plans over a symbolic representation of an object manipulation task, executes the plan using a geometric solver, and checks the plan's feasibility by taking into account the physical constraints of the scene, in an attempt to explain participants' actions and mental simulations when encountering such a task.
							<br>
							[<a href="https://arxiv.org/abs/1707.08212">Link</a>]
            </td>
            </td>
          </tr>

        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Teaching</h2>
              <ul>
								<li><i><a href="https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&academicYear=20232024&q=EE+276%3A+information+theory+tsachy&collapse=">Stanford EE 276: Information Theory</a></i>, Winter 2024: Teaching Assistant</li>
										taught by Professor <a href="https://web.stanford.edu/~tsachy/">Tsachy Weissman</a>
                  <br>
								<br>

								<li><i><a href="https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&academicYear=20222023&q=EE+276%3A+information+theory+tse&collapse=">Stanford EE 276: Information Theory</a></i>, Spring 2023: Teaching Assistant</li>
										taught by Professor <a href="https://tselab.stanford.edu/people/principal-investigator/david-tse/">David Tse</a>
                  <br>
								<br>
								<li><i><a href="https://explorecourses.stanford.edu/search?q=EE+178%3a+Probabilistic+Systems+Analysis&view=catalog&page=0&filter-coursestatus-Active=on&collapse=&academicYear=20222023">Stanford EE178: Probability Systems Analysis</a></i>, Fall 2022: Teaching Assistant</li>
										taught by <a href="https://kabirverchand.github.io/">Kabir Verchand</a> 								
                    <br>
								<br>
								<li><i><a href="https://stellar.mit.edu/S/course/6/fa19/6.438/">MIT 6.438: Algorithms for Inference</a></i>, Fall 2019: Teaching Assistant</li>
										taught by Professors <a href="https://idss.mit.edu/staff/guy-bresler/">Guy Bresler</a> and  <a href="http://allegro.mit.edu/~gww/">Gregory Wornell</a>
								<br>
								<br>
								<li><i><a href="http://web.mit.edu/bsaeed/www/matrix-stats/">MIT 6.S087: Matrices for Statistics</a></i>, Winter 2019, Winter 2020: Course Development & Lecturer</li>
								with Farrell Wu, Yang Yan, Hoi Wai Yu and Hung-Hsun Yu
								<br>
								<br>
								<li><i><a href="https://stellar.mit.edu/S/course/6/fa18/6.008/index.html">MIT 6.008: Introduction to Inference</a></i>, Fall 2018, Fall 2017: Laboratory & Teaching Assistant</li> 
								taught by Professors <a href="https://people.csail.mit.edu/polina/">Polina Golland</a> and <a href="http://allegro.mit.edu/~gww/">Gregory Wornell</a>
              </ul>
            </td>
          </tr>
          </tbody>
        </table>




        <!-- <p>* indicates equal contribution.</p> -->

        <!-- Website Credits -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <br>
                  <p align="right">
                    <font size="2">
                      <a href="http://jonbarron.info/">website template credits</a>
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        
        </td>
      </tr>
    </table>

</body>

</html>
